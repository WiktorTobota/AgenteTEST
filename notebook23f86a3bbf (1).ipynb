{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install google-generativeai langchain langchain-google-genai langgraph chromadb pymed Bio\n\n!pip install -Uq \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:25:07.869595Z","iopub.execute_input":"2025-04-16T21:25:07.869966Z","iopub.status.idle":"2025-04-16T21:25:16.469519Z","shell.execute_reply.started":"2025-04-16T21:25:07.869939Z","shell.execute_reply":"2025-04-16T21:25:16.468464Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\nRequirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\nRequirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.30)\nRequirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.5)\nRequirement already satisfied: pymed in /usr/local/lib/python3.11/dist-packages (0.8.9)\nRequirement already satisfied: Bio in /usr/local/lib/python3.11/dist-packages (1.7.1)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.16)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\nRequirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\nRequirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\nRequirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\nRequirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\nRequirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\nRequirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\nRequirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\nRequirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.25.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.0)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.27.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.27.0)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48b0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.27.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\nRequirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\nRequirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\nRequirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\nRequirement already satisfied: biopython>=1.80 in /usr/local/lib/python3.11/dist-packages (from Bio) (1.85)\nRequirement already satisfied: gprofiler-official in /usr/local/lib/python3.11/dist-packages (from Bio) (1.0.0)\nRequirement already satisfied: mygene in /usr/local/lib/python3.11/dist-packages (from Bio) (3.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from Bio) (2.2.3)\nRequirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from Bio) (1.8.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.19.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.67.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\nRequirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2.4.1)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\nRequirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\nRequirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\nRequirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\nRequirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (75.1.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\nRequirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.11/dist-packages (from mygene->Bio) (0.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2025.2)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->Bio) (4.3.7)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.26.4->langchain) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.26.4->langchain) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport google.generativeai as genai\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:05:51.621225Z","iopub.execute_input":"2025-04-16T21:05:51.621550Z","iopub.status.idle":"2025-04-16T21:05:51.627798Z","shell.execute_reply.started":"2025-04-16T21:05:51.621519Z","shell.execute_reply":"2025-04-16T21:05:51.626830Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom Bio import Entrez\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_key\")\nPubMed_API = UserSecretsClient().get_secret('PubMed')\n\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:25:29.349895Z","iopub.execute_input":"2025-04-16T21:25:29.350206Z","iopub.status.idle":"2025-04-16T21:25:29.468764Z","shell.execute_reply.started":"2025-04-16T21:25:29.350177Z","shell.execute_reply":"2025-04-16T21:25:29.467865Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"GEMINI_API_KEY = GOOGLE_API_KEY\n#PPLX_API_KEY = os.getenv(\"PPLX_API_KEY\") # Perplexity API Key\n\n# --- Klucze API Narzędzi ---\nFIRECRAWL_API_KEY = os.getenv(\"FIRECLAWL_API_KEY\") \nPUBMED_API_KEY = PubMed_API \n\n# --- Inne Konfiguracje ---\n# PubMed (Entrez) wymaga adresu email\nPUBMED_EMAIL = \"wiktortobota13@Gmail.com\" \n\n\nprint(f\"GEMINI_API_KEY loaded: {'Yes' if GEMINI_API_KEY else 'No'}\")\nprint(f\"FIRECRAWL_API_KEY loaded: {'Yes' if FIRECRAWL_API_KEY else 'No'}\")\nprint(f\"PUBMED_EMAIL set to: {PUBMED_EMAIL}\")\n\n# Możesz dodać tu logikę wyboru domyślnego LLM, jeśli chcesz\nDEFAULT_LLM_PROVIDER = \"gemini\" if GEMINI_API_KEY else \"openai\" if OPENAI_API_KEY else None\nprint(f\"Default LLM Provider based on available keys: {DEFAULT_LLM_PROVIDER}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:14:49.111206Z","iopub.execute_input":"2025-04-16T21:14:49.111530Z","iopub.status.idle":"2025-04-16T21:14:49.118314Z","shell.execute_reply.started":"2025-04-16T21:14:49.111508Z","shell.execute_reply":"2025-04-16T21:14:49.117392Z"}},"outputs":[{"name":"stdout","text":"GEMINI_API_KEY loaded: Yes\nFIRECRAWL_API_KEY loaded: No\nPUBMED_EMAIL set to: wiktortobota13@Gmail.com\nDefault LLM Provider based on available keys: gemini\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"Entrez.email   = PUBMED_EMAIL\nEntrez.api_key = PUBMED_API_KEY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:13:42.371057Z","iopub.execute_input":"2025-04-16T21:13:42.371367Z","iopub.status.idle":"2025-04-16T21:13:42.376609Z","shell.execute_reply.started":"2025-04-16T21:13:42.371343Z","shell.execute_reply":"2025-04-16T21:13:42.375231Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"\ndef generate_completion(prompt: str, model_preference: str = \"gemini\", max_tokens: int = 1000) -> str:\n    \"\"\"\n    Generuje uzupełnienie tekstu używając preferowanego dostawcy LLM.\n\n    Args:\n        prompt (str): Tekst wejściowy dla LLM.\n        model_preference (str): Preferowany dostawca ('gemini' lub 'openai').\n        max_tokens (int): Maksymalna liczba tokenów w odpowiedzi.\n\n    Returns:\n        str: Wygenerowany tekst lub komunikat błędu.\n    \"\"\"\n\n    try:\n        # Wybierz odpowiedni model Gemini (np. gemini-pro)\n        model = genai.GenerativeModel('gemini-1.5-flash-002')\n        response = model.generate_content(prompt)\n        # Sprawdź czy odpowiedź zawiera tekst (może być zablokowana)\n        if response.parts:\n             return response.text\n        else:\n             # Sprawdź przyczynę braku odpowiedzi (np. blokada bezpieczeństwa)\n             print(f\"Ostrzeżenie: Odpowiedź Gemini nie zawiera tekstu. Powód: {response.prompt_feedback}\")\n             return f\"Błąd: Odpowiedź Gemini była pusta lub zablokowana ({response.prompt_feedback}).\"\n    except Exception as e:\n        print(f\"Błąd podczas komunikacji z Gemini API: {e}\")\n        return f\"Błąd Gemini API: {e}\"\n\n\n\n# --- Przykładowe użycie (można uruchomić ten plik bezpośrednio do testów) ---\nif __name__ == '__main__':\n    test_prompt = \"Napisz krótki opis działania sieci neuronowej.\"\n    print(\"\\nTestowanie generowania LLM:\")\n\n    print(\"\\n--- Test Gemini ---\")\n    gemini_response = generate_completion(test_prompt, model_preference=\"gemini\")\n    print(gemini_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:13:48.937935Z","iopub.execute_input":"2025-04-16T21:13:48.938219Z","iopub.status.idle":"2025-04-16T21:13:50.588557Z","shell.execute_reply.started":"2025-04-16T21:13:48.938197Z","shell.execute_reply":"2025-04-16T21:13:50.587705Z"}},"outputs":[{"name":"stdout","text":"\nTestowanie generowania LLM:\n\n--- Test Gemini ---\nSieć neuronowa to system obliczeniowy inspirowany strukturą i działaniem ludzkiego mózgu. Składa się z połączonych ze sobą węzłów (neuronów) ułożonych w warstwach: wejściowej, ukrytych i wyjściowej.  Informacja w postaci danych wejściowych jest przekazywana przez sieć, gdzie każdy neuron przetwarza ją, stosując wagę przypisaną do każdej połączenia.  Te wagi są modyfikowane w procesie uczenia, w którym sieć dostosowuje swoje parametry na podstawie porównania wyników z danymi oczekiwanymi.  Dzięki temu sieć uczy się rozpoznawać wzorce i przewidywać wyniki na podstawie nowych, nieznanych danych.  Im więcej danych treningowych i im lepiej zaprojektowana sieć, tym dokładniejsze są jej przewidywania.\n\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"#tu chwilowo nie mam API\nfrom typing import List, Dict, Any, Optional\n\ndef search_firecrawl(query: str, fetch_content: bool = True, max_results: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"Wykonuje wyszukiwanie za pomoc¹ Firecrawl.\"\"\"\n    if not firecrawl_client:\n        print(\"B³¹d: Klient Firecrawl jest nieaktywny.\")\n        return []\n    try:\n        print(f\"Wysy³anie zapytania do Firecrawl: {query}\")\n        # Wywo³anie API Firecrawl - BEZ limitowania wyników tutaj, jeœli API go nie wspiera bezpoœrednio\n        # (parametr max_results tutaj jest tylko dla informacji, nie u¿ywamy go do krojenia poni¿ej)\n        results = firecrawl_client.search(\n            query,\n            params={'pageOptions': {'fetchPageContent': fetch_content}}\n        )\n        # SprawdŸ typ wyniku na wszelki wypadek\n        if not isinstance(results, list):\n             print(f\"Ostrze¿enie: Firecrawl API zwróci³o nieoczekiwany typ danych ({type(results)}), oczekiwano listy.\")\n             # Spróbuj obs³u¿yæ, jeœli to mo¿liwe, lub zwróæ pust¹ listê\n             if isinstance(results, dict) and 'data' in results and isinstance(results['data'], list):\n                  results = results['data'] # Próba odzyskania listy z typowej struktury API\n             else:\n                  return [] # Zwróæ pust¹ listê, jeœli nie mo¿na przetworzyæ\n\n        print(f\"Otrzymano {len(results)} wyników z Firecrawl dla zapytania '{query}'\")\n\n        formatted_results = []\n        # Iteruj po WSZYSTKICH wynikach zwróconych przez API\n        for res in results: # <--- Iterujemy po results_list\n            if isinstance(res, dict):\n                # U¿yj 'title' jeœli jest, inaczej spróbuj 'name' (standardowo Firecrawl zwraca 'title')\n                # U¿yj 'description' jeœli jest, inaczej spróbuj 'snippet'\n                # U¿yj 'markdown' lub 'content' jeœli jest, inaczej None\n                title = res.get('title') or res.get('name')\n                snippet = res.get('description') or res.get('snippet')\n                content = None\n                if fetch_content:\n                    content = res.get('markdown') or res.get('content') # Dodaj inne mo¿liwe klucze treœci, jeœli znasz\n\n                formatted_results.append({\n                    'url': res.get('url'),\n                    'title': title,\n                    'snippet': snippet,\n                    'content': content,\n                    'source': 'firecrawl_search'\n                })\n            else:\n                 print(f\"Ostrze¿enie: Pomiêto nieprawid³owy element wyniku Firecrawl: {res}\")\n\n        return formatted_results\n    except Exception as e:\n        print(f\"B³¹d podczas wyszukiwania w Firecrawl dla zapytania '{query}': {e}\")\n        # Dodajmy traceback dla lepszego debugowania\n        import traceback\n        traceback.print_exc()\n        return []\n\n# --- PubMed (Entrez) Client ---\ntry:\n    from Bio import Entrez\n    Entrez.email = PUBMED_EMAIL\n    if not Entrez.email or Entrez.email == \"wiktortobota13@gmail.com\":\n        print(\"Ostrze¿enie: Nie ustawiono prawid³owego adresu email dla PubMed Entrez w .env (PUBMED_EMAIL).\")\n        entrez_active = False\n    else:\n         print(f\"Klient PubMed Entrez skonfigurowany z emailem: {Entrez.email}\")\n         entrez_active = True\n\nexcept ImportError:\n    Entrez = None\n    entrez_active = False\n    print(\"Ostrze¿enie: Biblioteka biopython nie jest zainstalowana. PubMed Entrez nie bêdzie dzia³aæ.\")\n\n\ndef search_pubmed_entrez(query: str, max_results: int = 5) -> Optional[List[Dict[str, Any]]]:\n    \"\"\"Wyszukuje w PubMed u¿ywaj¹c Bio.Entrez.\"\"\"\n    if not entrez_active or not Entrez:\n        print(\"Nie mo¿na u¿yæ PubMed API (Entrez): Klient nieaktywny lub brak biblioteki/emaila.\")\n        return None # Zwróæ None, aby wskazaæ problem z t¹ metod¹\n\n    try:\n        print(f\"Wysy³anie zapytania do PubMed (Entrez): {query}\")\n        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=str(max_results), api_key=PUBMED_API_KEY) # Dodano opcjonalny klucz API\n        record = Entrez.read(handle)\n        handle.close()\n        pmids = record[\"IdList\"]\n\n        if not pmids:\n            print(\"PubMed (Entrez): Nie znaleziono artyku³ów.\")\n            return [] # Zwróæ pust¹ listê, jeœli nie ma wyników\n\n        print(f\"PubMed (Entrez): Znaleziono {len(pmids)} ID artyku³ów.\")\n        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"xml\", api_key=PUBMED_API_KEY) # Dodano opcjonalny klucz API\n        records = Entrez.read(handle)\n        handle.close()\n\n        articles = []\n        pubmed_articles = records.get('PubmedArticle', [])\n        if isinstance(pubmed_articles, dict): pubmed_articles = [pubmed_articles]\n\n        for article_data in pubmed_articles:\n             medline_citation = article_data.get('MedlineCitation')\n             if not medline_citation: continue\n             article_info = medline_citation.get('Article', {})\n             title = article_info.get('ArticleTitle', 'Brak tytu³u')\n             abstract_dict = article_info.get('Abstract', {})\n             abstract = abstract_dict.get('AbstractText', [''])[0]\n\n             doi = None\n             article_ids = article_info.get('ELocationID', [])\n             if not isinstance(article_ids, list): article_ids = [article_ids]\n             for identifier in article_ids:\n                  if identifier.attributes.get('EIdType') == 'doi':\n                       doi = str(identifier)\n                       break\n             if not doi:\n                  pubmed_data = article_data.get('PubmedData', {})\n                  article_id_list = pubmed_data.get('ArticleIdList', [])\n                  if isinstance(article_id_list, dict): article_id_list = article_id_list.get('ArticleId', [])\n                  if not isinstance(article_id_list, list): article_id_list = [article_id_list]\n                  for identifier in article_id_list:\n                       if identifier.attributes.get('IdType') == 'doi':\n                            doi = str(identifier)\n                            break\n\n             pmid = medline_citation.get('PMID', '')\n             authors_list = article_info.get('AuthorList', [])\n             authors = []\n             if authors_list:\n                  for author_entry in authors_list:\n                       if isinstance(author_entry, dict):\n                            lastname = author_entry.get('LastName', '')\n                            forename = author_entry.get('ForeName', '')\n                            if lastname and forename: authors.append(f\"{forename} {lastname}\")\n\n             articles.append({\n                 'title': str(title), 'authors': authors, 'abstract': str(abstract),\n                 'doi': doi, 'pmid': str(pmid),\n                 'url': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" if pmid else None,\n                 'source': 'pubmed_entrez'\n             })\n        print(f\"PubMed (Entrez): Pomyœlnie przetworzono {len(articles)} artyku³ów.\")\n        return articles\n\n    except Exception as e:\n        print(f\"B³¹d podczas wyszukiwania w PubMed (Entrez): {e}\")\n        # Mo¿na logowaæ szczegó³y b³êdu\n        # Sprawdzenie specyficznych b³êdów Entrez (np. 429 Too Many Requests)\n        if \"HTTP Error 429\" in str(e):\n             print(\"B³¹d Entrez: Przekroczono limit zapytañ. Spróbuj ponownie póŸniej.\")\n        return None # Zwróæ None w przypadku b³êdu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:15:53.932284Z","iopub.execute_input":"2025-04-16T21:15:53.932741Z","iopub.status.idle":"2025-04-16T21:15:53.953137Z","shell.execute_reply.started":"2025-04-16T21:15:53.932711Z","shell.execute_reply":"2025-04-16T21:15:53.951953Z"}},"outputs":[{"name":"stdout","text":"Klient PubMed Entrez skonfigurowany z emailem: wiktortobota13@Gmail.com\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"from typing import List, Dict, Any\n\n\nclass WebSearchAgent:\n    def _generate_queries(self, subgraph: Any, prompt: str, num_queries: int = 3) -> List[str]:\n        \"\"\"Generuje zapytania do wyszukiwarki używając LLM.\"\"\"\n        # Przygotuj prompt dla LLM\n        # TODO: Ulepszyć ekstrakcję informacji z subgraph\n        subgraph_str = str(subgraph) # Prosta konwersja na string, można ulepszyć\n        llm_prompt = f\"\"\"\n        Na podstawie poniższego podgrafu wiedzy i głównego zapytania użytkownika, wygeneruj {num_queries} różnorodnych zapytań do ogólnej wyszukiwarki internetowej (np. Google).\n        Zapytania powinny pomóc znaleźć ogólne informacje, kontekst, wiadomości i potencjalnie powiązane koncepcje.\n        Format odpowiedzi: Każde zapytanie w nowej linii, bez numeracji.\n\n        Podgraf:\n        {subgraph_str}\n\n        Główne zapytanie użytkownika:\n        {prompt}\n\n        Wygenerowane zapytania:\n        \"\"\"\n        print(\"Generowanie zapytań dla WebSearch używając LLM...\")\n        raw_queries = generate_completion(llm_prompt)\n\n        if raw_queries.startswith(\"Błąd:\") or not raw_queries.strip():\n             print(f\"Nie udało się wygenerować zapytań przez LLM. Używanie domyślnego zapytania. Błąd: {raw_queries}\")\n             return [prompt] # Fallback\n\n        # Przetwarzanie odpowiedzi LLM\n        queries = [q.strip() for q in raw_queries.strip().split('\\n') if q.strip()]\n        print(f\"Wygenerowane zapytania przez LLM: {queries}\")\n        return queries if queries else [prompt] # Zwróć wygenerowane lub fallback\n\n    def search(self, subgraph: Any, prompt: str, max_results_per_query: int = 3) -> List[Dict[str, Any]]:\n        \"\"\"Przeszukuje internet używając Firecrawl z zapytaniami generowanymi przez LLM.\"\"\"\n        queries = self._generate_queries(subgraph, prompt)\n        all_results = []\n\n        for query in queries:\n            # Użyj funkcji z api_clients\n            results = search_firecrawl(query, fetch_content=True, max_results=max_results_per_query)\n            all_results.extend(results)\n            # Opcjonalnie: Zastosuj LLM do podsumowania/ekstrakcji z 'content' wyników, jeśli potrzebne\n            # for result in results:\n            #     if result.get('content'):\n            #         summary_prompt = f\"Podsumuj poniższy tekst w kontekście zapytania '{query}':\\n\\n{result['content'][:2000]}\" # Ogranicz długość\n            #         summary = generate_completion(summary_prompt)\n            #         result['llm_summary'] = summary # Dodaj podsumowanie do wyniku\n\n        total_results = len(all_results)\n        print(f\"WebSearchAgent zakończył pracę, zwracając {total_results} wyników.\")\n        # Można dodać logikę deduplikacji wyników, jeśli wiele zapytań zwraca te same URL\n        # unique_results = {r['url']: r for r in all_results}.values()\n        # print(f\"Po deduplikacji pozostało {len(unique_results)} wyników.\")\n        # return list(unique_results)\n        return all_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:15:57.130290Z","iopub.execute_input":"2025-04-16T21:15:57.130624Z","iopub.status.idle":"2025-04-16T21:15:57.139922Z","shell.execute_reply.started":"2025-04-16T21:15:57.130598Z","shell.execute_reply":"2025-04-16T21:15:57.138917Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"\nfrom typing import List, Dict, Any, Optional\nimport re # Import re do prostego czyszczenia tekstu dla fallbacku\n\nclass SLSAgent:\n    def __init__(self, use_pubmed_api: bool = True):\n        self.use_pubmed_api = use_pubmed_api\n        print(f\"SLSAgent skonfigurowany do użycia PubMed API: {self.use_pubmed_api}\")\n\n    def _generate_scientific_queries(self, input_text: str, num_queries: int = 2) -> List[str]:\n        \"\"\"Generuje zapytania do wyszukiwarek naukowych używając LLM na podstawie tekstu wejściowego.\"\"\"\n\n        llm_prompt = f\"\"\"\n                Na podstawie poniższego tekstu analitycznego, zidentyfikuj kluczowe pojęcia biomedyczne (np. z sekcji Definitions) i relacje (np. z sekcji Relationships). Następnie wygeneruj {num_queries} zapytań zoptymalizowanych dla wyszukiwarek literatury naukowej, takich jak PubMed. Używaj precyzyjnych terminów, operatorów logicznych (AND, OR, NOT) i ewentualnie formatowania specyficznego dla PubMed (np. [Title/Abstract], [MeSH Terms]). Skup się na znalezieniu relevantnych publikacji naukowych potwierdzających, rozwijających lub kwestionujących informacje z tekstu wejściowego.\n                Format odpowiedzi: Każde zapytanie w nowej linii, bez numeracji.\n\n                Tekst Wejściowy:\n                ```\n                {input_text}\n                ```\n\n                Wygenerowane zapytania (format PubMed):\n        \"\"\"\n        print(\"Generowanie zapytań dla SLS używając LLM...\")\n        raw_queries = generate_completion(llm_prompt)\n\n        # --- POPRAWIONA LOGIKA FALLBACK ---\n        if raw_queries.startswith(\"Błąd:\") or not raw_queries.strip():\n             print(f\"Nie udało się wygenerować zapytań naukowych przez LLM. Używanie fallbacku opartego na input_text. Błąd: {raw_queries}\")\n             # Prosty fallback: weź pierwsze ~10 słów z input_text jako zapytanie\n             # Usuń znaki specjalne, weź unikalne słowa\n             words = re.findall(r'\\b\\w+\\b', input_text.lower())[:15] # Weź pierwsze 15 słów\n             fallback_query = \" \".join(list(dict.fromkeys(words))) # Unikalne słowa w kolejności\n             print(f\"Wygenerowano zapytanie fallback: {fallback_query}\")\n             # Zwróć listę z jednym zapytaniem fallback lub pustą listę, jeśli tekst był pusty\n             return [fallback_query] if fallback_query else []\n\n        queries = [q.strip() for q in raw_queries.strip().split('\\n') if q.strip()]\n        print(f\"Wygenerowane zapytania naukowe przez LLM: {queries}\")\n        # --- POPRAWIONA WARTOŚĆ ZWRACANA ---\n        # Jeśli LLM zwrócił pustą listę (mimo że nie było błędu), też użyj fallbacku\n        if not queries:\n             words = re.findall(r'\\b\\w+\\b', input_text.lower())[:15]\n             fallback_query = \" \".join(list(dict.fromkeys(words)))\n             print(f\"LLM zwrócił puste zapytania. Wygenerowano zapytanie fallback: {fallback_query}\")\n             return [fallback_query] if fallback_query else []\n        else:\n             return queries\n\n\n    def search_papers(self, input_text: str, max_results_per_query: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Wyszukuje artykuły używając PubMed API i/lub Firecrawl z zapytaniami LLM.\"\"\"\n        # --- POPRAWIONE WYWOŁANIE _generate_scientific_queries ---\n        queries = self._generate_scientific_queries(input_text=input_text) # Przekazujemy input_text\n\n        if not queries: # Jeśli generowanie zapytań (nawet fallback) zawiodło\n             print(\"SLSAgent: Nie udało się wygenerować żadnych zapytań. Zwracam pustą listę.\")\n             return []\n\n        all_articles = []\n        pubmed_failed_or_disabled = not self.use_pubmed_api\n\n        for query in queries:\n            articles_pubmed = None\n            # 1. Spróbuj PubMed Entrez, jeśli włączone\n            if self.use_pubmed_api:\n                print(f\"SLSAgent: Wyszukiwanie w PubMed dla zapytania: '{query}'\") # Dodano log\n                articles_pubmed = search_pubmed_entrez(query, max_results=max_results_per_query)\n                if articles_pubmed is not None:\n                    all_articles.extend(articles_pubmed)\n                    print(f\"Dodano {len(articles_pubmed)} artykułów z PubMed dla zapytania '{query}'.\")\n                else:\n                    print(f\"PubMed Entrez zwrócił błąd dla zapytania '{query}'.\")\n                    pubmed_failed_or_disabled = True\n\n            # 2. Użyj Firecrawl jako fallback\n            if pubmed_failed_or_disabled or (self.use_pubmed_api and not articles_pubmed and articles_pubmed is not None):\n                # Dodano warunek 'articles_pubmed is not None', aby nie uruchamiać Firecrawl, jeśli Entrez miał błąd, chyba że Entrez jest wyłączony\n                 if not articles_pubmed and articles_pubmed is not None:\n                      print(f\"PubMed nie znalazł wyników dla '{query}'. Próba za pomocą Firecrawl.\")\n                 elif pubmed_failed_or_disabled and self.use_pubmed_api: # Jeśli Entrez miał błąd\n                       print(f\"Próba wyszukania naukowego za pomocą Firecrawl z powodu błędu Entrez dla zapytania: {query}\")\n                 elif not self.use_pubmed_api: # Jeśli Entrez był wyłączony\n                        print(f\"Próba wyszukania naukowego za pomocą Firecrawl (PubMed API wyłączone) dla zapytania: {query}\")\n\n                # firecrawl_query = f\"{query} site:pubmed.ncbi.nlm.nih.gov OR site:scholar.google.com OR site:biorxiv.org OR site:medrxiv.org OR site:arxiv.org\"\n                # articles_firecrawl = search_firecrawl(firecrawl_query, fetch_content=False) # Domyślnie nie pobieraj treści\n                # if articles_firecrawl:\n                 #   for article in articles_firecrawl:\n                 #       article['source'] = 'firecrawl_scientific_search'\n                #    all_articles.extend(articles_firecrawl)\n                #    print(f\"Dodano {len(articles_firecrawl)} potencjalnych artykułów z Firecrawl dla zapytania '{query}'.\")\n                # else:\n                #      print(f\"Firecrawl nie znalazł wyników dla zapytania: '{firecrawl_query}'\")\n\n\n        total_articles = len(all_articles)\n        print(f\"SLSAgent zakończył pracę, zwracając łącznie {total_articles} potencjalnych artykułów.\")\n        # TODO: Deduplikacja wyników (np. na podstawie DOI lub tytułu)\n        # TODO: Opcjonalna ocena relevancji przez LLM na podstawie abstraktów/snippetów\n        return all_articles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:15:09.233850Z","iopub.execute_input":"2025-04-16T21:15:09.234167Z","iopub.status.idle":"2025-04-16T21:15:09.249959Z","shell.execute_reply.started":"2025-04-16T21:15:09.234143Z","shell.execute_reply":"2025-04-16T21:15:09.248978Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"\nclass ContextBuilderAgent:\n    def __init__(self):\n        print(\"Inicjalizacja ContextBuilderAgent...\")\n        # Inicjalizacja specjalistycznych agentów (bez Aggregatora)\n        self.web_searcher = WebSearchAgent()\n        self.sls_searcher = SLSAgent(use_pubmed_api=True)\n        # self.kg_constructor = KnowledgeGraphConstructorAgent()\n        # Usunięto self.aggregator\n        print(\"Agenci podrzędni (Web, SLS) zainicjalizowani.\") # Usunięto Aggregator z logu\n\n    # --- Metody skopiowane z ContextAggregatorAgent ---\n\n    def _prepare_llm_prompt(self, context_data: Dict[str, Any]) -> str:\n        \"\"\"Przygotowuje prompt dla LLM na podstawie zebranych danych.\"\"\"\n\n        initial_text = context_data.get('initial_input_text', 'Brak tekstu wejściowego.')\n\n        web_info_parts = []\n        # Zakładamy, że web_search_results jest pustą listą, ale zostawiamy logikę na przyszłość\n        for res in context_data.get('web_search_results', []):\n            info = res.get('title') if res.get('title') else res.get('snippet')\n            url = res.get('url', 'N/A')\n            if info:\n                web_info_parts.append(f\"- {info} (URL: {url})\")\n        web_info_str = \"\\n\".join(web_info_parts) if web_info_parts else \"Brak wyników z wyszukiwania w internecie (funkcjonalność wyłączona lub brak wyników).\" # Zaktualizowano komunikat\n\n        literature_abstracts = []\n        for res in context_data.get('scientific_literature_results', []):\n            abstract = res.get('abstract')\n            title = res.get('title')\n            url = res.get('url', 'N/A')\n            pmid = res.get('pmid', 'N/A')\n            if abstract:\n                # Używamy \\n dla nowej linii w f-string, spacja może nie być potrzebna\n                literature_abstracts.append(f\"- {title} (PMID: {pmid}, URL: {url}):\\n{abstract}\")\n            elif title:\n                 literature_abstracts.append(f\"- {title} (PMID: {pmid}, URL: {url}) (brak abstraktu)\")\n        literature_info_str = \"\\n\\n\".join(literature_abstracts) if literature_abstracts else \"Brak wyników z literatury naukowej.\"\n\n        llm_prompt = f\"\"\"\n        Zadanie: Jesteś specjalistą analizującym dane biomedyczne. Na podstawie poniższego **Tekstu Wejściowego** oraz dodatkowych informacji znalezionych w **Internecie** i **Literaturze Naukowej**, wygeneruj szczegółowe, ustrukturyzowane i obiektywne podsumowanie kontekstu w języku angielskim. Zachowaj neutralny ton. Odpowiedź powinna ściśle trzymać się dostarczonych informacji i mieć następującą strukturę:\n\n        **1. Analizowany Temat:**\n        (Krótkie, 1-2 zdaniowe podsumowanie głównych pojęć i relacji opisanych w Tekście Wejściowym.)\n\n        **2. Kluczowe Definicje:**\n        (Wylistuj definicje pojęć zawarte w Tekście Wejściowym. Jeśli definicji brakuje w tekście wejściowym, ale została znaleziona w źródłach, możesz ją dodać z adnotacją źródła, np. [wg. PubMed: ID].)\n        - Pojęcie 1: Definicja...\n        - Pojęcie 2: Definicja...\n        - ...\n\n        **3. Główne Powiązania/Mechanizmy Opisane w Tekście Wejściowym:**\n        (Wylistuj i opisz zwięźle relacje wspomniane w Tekście Wejściowym. Możesz wzbogacić opis o 1-2 zdania kontekstu z znalezionych źródeł, jeśli bezpośrednio dotyczą tej relacji.)\n        - Relacja 1 (np. Zapalenie zwiększa Amyloid Beta): Opis relacji... (ewentualne wzbogacenie ze źródeł)\n        - Relacja 2 (np. Amyloid Beta akumuluje się w Ch. Alzheimera): Opis...\n        - ...\n\n        **4. Kontekst z Literatury Naukowej:**\n        (Syntetyczny przegląd najważniejszych informacji i wniosków z abstraktów naukowych. Pogrupuj informacje tematycznie, jeśli to możliwe. Unikaj bezpośredniego cytowania poszczególnych artykułów, skup się na zagregowanej wiedzy.)\n\n        **5. Kontekst z Ogólnego Internetu:**\n        (Podsumowanie relevantnych informacji z tytułów/snippetów znalezionych w Internecie. Wskaż główne punkty lub perspektywy. Pomiń tę sekcję, jeśli brak wyników lub informacji.)\n\n        **6. Synteza i Wnioski Ogólne:**\n        (Krótkie, 2-4 zdaniowe podsumowanie łączące wszystkie zebrane informacje. Jakie są główne wnioski płynące z analizy tekstu wejściowego w świetle znalezionego kontekstu?)\n\n        --- Dane Wejściowe dla LLM ---\n\n        **Tekst Wejściowy do Analizy:**\n        ```\n        {initial_text}\n        ```\n\n        **Informacje z Internetu (Tytuły/Fragmenty):**\n        ```\n        {web_info_str}\n        ```\n\n        **Informacje z Literatury Naukowej (Tytuły/Abstrakty):**\n        ```\n        {literature_info_str}\n        ```\n\n        --- Koniec Danych Wejściowych ---\n\n        Wygenerowane Podsumowanie Kontekstu (w formacie opisanym powyżej):\n        \"\"\"\n        return llm_prompt\n\n    def generate_aggregated_context(self, context_data: Dict[str, Any]) -> str:\n        \"\"\"Generuje zagregowany kontekst tekstowy używając LLM.\"\"\"\n        print(\"ContextBuilderAgent: Generowanie zagregowanego kontekstu...\") # Zmieniono nazwę agenta w logu\n\n        has_literature_results = bool(context_data.get('scientific_literature_results'))\n        # Można też sprawdzić web_search_results, jeśli zostaną włączone i naprawione\n        # has_web_results = bool(context_data.get('web_search_results'))\n        # if not has_literature_results and not has_web_results:\n\n        if not has_literature_results: # Na razie sprawdzamy tylko literaturę\n             print(\"Ostrzeżenie (ContextBuilderAgent): Brak wyników z literatury. Zwracam podstawowy kontekst.\")\n             return f\"Analiza tekstu wejściowego wykazała następujące pojęcia i relacje:\\n{context_data.get('initial_input_text', '')}\\nNie znaleziono dodatkowego kontekstu w dostępnych źródłach (Literatura Naukowa, Internet).\"\n\n        llm_prompt = self._prepare_llm_prompt(context_data)\n        print(\"\\n--- Prompt dla generowania zagregowanego kontekstu ---\") # Zmieniono nazwę logu\n        print(llm_prompt)\n        print(\"--- Koniec Promptu dla generowania zagregowanego kontekstu ---\\n\")\n\n        # Wywołujemy generate_completion bezpośrednio\n        aggregated_context = generate_completion(llm_prompt, model_preference=\"gemini\", max_tokens=3000)\n\n        if aggregated_context.startswith(\"Błąd:\") or not aggregated_context.strip():\n            print(f\"Błąd (ContextBuilderAgent): LLM nie zwrócił poprawnego kontekstu. Błąd: {aggregated_context}\")\n            return f\"Wystąpił błąd podczas generowania zagregowanego kontekstu przez LLM: {aggregated_context}\"\n        else:\n            print(\"ContextBuilderAgent: Pomyślnie wygenerowano zagregowany kontekst.\")\n            return aggregated_context.strip()\n\n    # --- Główna metoda build_context ---\n\n    def build_context(self, input_text: str) -> Dict[str, Any]:\n        \"\"\"Buduje kontekst, w tym zagregowane podsumowanie tekstowe.\"\"\"\n        print(\"\\n=== ContextBuilderAgent: Rozpoczynanie budowania kontekstu ===\")\n\n        # Web search jest wyłączony zgodnie z ustaleniami\n        # print(\"\\n--- Uruchamianie WebSearchAgent ---\")\n        # web_results = self.web_searcher.search(input_text=input_text)\n        web_results = []\n        print(f\"ContextBuilderAgent: Pominięto WebSearchAgent (web_results ustawione na pustą listę).\")\n\n        print(\"\\n--- Uruchamianie SLSAgent ---\")\n        scientific_results = self.sls_searcher.search_papers(input_text=input_text)\n        print(f\"ContextBuilderAgent: Otrzymano {len(scientific_results)} wyników z SLSAgent.\")\n\n        # --- Tutaj można dodać wywołanie KG Agenta ---\n\n        # Stworzenie wstępnego kontekstu\n        context = {\n            \"initial_input_text\": input_text,\n            #\"web_search_results\": web_results, # Pusta lista\n            \"scientific_literature_results\": scientific_results,\n            # \"knowledge_graph\": kg_results\n        }\n\n        # --- Generowanie Zagregowanego Kontekstu (bezpośrednie wywołanie metody) ---\n        # print(\"\\n--- Uruchamianie ContextAggregatorAgent ---\") # Usunięto log o osobnym agencie\n        # Zamiast: aggregated_context_text = self.aggregator.generate_aggregated_context(context)\n        # Używamy:\n        aggregated_context_text = self.generate_aggregated_context(context) # <--- BEZPOŚREDNIE WYWOŁANIE\n        context['aggregated_context'] = aggregated_context_text # Dodajemy wynik do słownika\n\n        print(\"\\n=== ContextBuilderAgent: Zakończono budowanie kontekstu (z agregacją) ===\")\n        return context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:15:12.481864Z","iopub.execute_input":"2025-04-16T21:15:12.482148Z","iopub.status.idle":"2025-04-16T21:15:12.497680Z","shell.execute_reply.started":"2025-04-16T21:15:12.482127Z","shell.execute_reply":"2025-04-16T21:15:12.496641Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"import json\n\ndef run_context_building():\n    \"\"\"Główna funkcja uruchamiaj�ca proces budowania kontekstu.\"\"\"\n\n    print(\"=\"*50)\n    print(\" Rozpoczynanie procesu budowania kontekstu\")\n    print(\"=\"*50)\n\n    # Przyk�adowy subgraph i prompt\n   # example_subgraph = {\n   #     \"nodes\": [{\"id\": \"1\", \"name\": \"Chronic Kidney Disease\"}, {\"id\": \"2\", \"name\": \"Anemia\"}],\n   #     \"edges\": [{\"source\": \"1\", \"target\": \"2\", \"relation\": \"associated_with\"}]\n   # }\n    #with open('/home/wiktor/hackathon/hackathon/sample_subgraph.json', 'r') as f:\n    #    example_subgraph = json.load(f)\n\n    #example_prompt = \"Investigate how inflammation influences the development of Alzheimer�s disease by analyzing the mechanisms related to amyloid beta accumulation, the role of tau protein, and microglial activation, as illustrated in the provided knowledge graph.\"\n\n    analysis_text = \"\"\"# Ontologist Analysis\n                    {\n                    ### Definitions:\n                    - **Inflammation**: A biological response of body tissues to harmful stimuli, such as pathogens, damaged cells, or irritants. It involves immune cell activation, blood vessel dilation, and the release of various molecules that promote healing.\n                    - **Amyloid Beta**: A peptide that is produced in the brain through the breakdown of a larger protein called amyloid precursor protein (APP). Accumulation of amyloid beta in the brain is commonly associated with neurodegenerative diseases, especially Alzheimer's disease.\n                    - **Alzheimer's Disease**: A progressive neurological disorder characterized by cognitive decline, memory loss, and changes in behavior and personality. It is associated with the accumulation of amyloid beta plaques and neurofibrillary tangles in the brain.\n\n                    ### Relationships\n                    - **Inflammation increases Amyloid Beta**: This relationship indicates that the presence of inflammation in the brain can lead to an increase in the production or aggregation of amyloid beta peptides. This connection suggests that inflammatory processes might play a crucial role in the pathogenesis of Alzheimer's disease by enhancing the toxic effects of amyloid beta.\n                    ###\n                    - **Amyloid Beta accumulates in Alzheimer's Disease**: This relationship illustrates how the accumulation of amyloid beta is a defining feature of Alzheimer's disease. In patients with Alzheimer�s, there is a significant build-up of amyloid beta plaques in the brain, which is believed to contribute to the neurodegenerative processes observed in the disorder, such as synaptic dysfunction and neuronal death.\n                    }\"\"\"\n\n    print(\"\\nDane wej�ciowe:\")\n    #print(f\"  Subgraph: {json.dumps(example_subgraph, indent=2)}\")\n    #print(f\"  Prompt: {example_prompt}\")\n    print(f\" Tekst wej�ciowy analizy: {analysis_text}\")\n    # Utworzenie i uruchomienie g��wnego agenta\n    context_builder = ContextBuilderAgent()\n    final_context = context_builder.build_context(input_text=analysis_text)\n\n    print(\"\\n\" + \"=\"*50)\n    print(\" Finalny Kontekst Zosta� Zbudowany\")\n    print(\"=\"*50)\n\n    # Wy�wietlenie podsumowania wynik�w\n    print(f\"Liczba wynik�w z wyszukiwania web: {len(final_context.get('web_search_results', []))}\")\n    print(f\"Liczba wynik�w z literatury naukowej: {len(final_context.get('scientific_literature_results', []))}\")\n\n    # Opcjonalnie: Zapisz kontekst do pliku JSON\n    try:\n        output_filename = \"final_context.json\"\n        with open(output_filename, 'w', encoding='utf-8') as f:\n            json.dump(final_context, f, indent=4, ensure_ascii=False)\n        print(f\"\\nPe�ny kontekst zapisano do pliku: {output_filename}\")\n    except Exception as e:\n        print(f\"\\nB��d podczas zapisywania kontekstu do pliku: {e}\")\n\n    # Mo�esz tutaj doda� kod do dalszego przetwarzania `final_context`\n\nif __name__ == \"__main__\":\n    run_context_building()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:16:01.452361Z","iopub.execute_input":"2025-04-16T21:16:01.452780Z","iopub.status.idle":"2025-04-16T21:16:07.900615Z","shell.execute_reply.started":"2025-04-16T21:16:01.452756Z","shell.execute_reply":"2025-04-16T21:16:07.899731Z"}},"outputs":[{"name":"stdout","text":"==================================================\n Rozpoczynanie procesu budowania kontekstu\n==================================================\n\nDane wej�ciowe:\n Tekst wej�ciowy analizy: # Ontologist Analysis\n                    {\n                    ### Definitions:\n                    - **Inflammation**: A biological response of body tissues to harmful stimuli, such as pathogens, damaged cells, or irritants. It involves immune cell activation, blood vessel dilation, and the release of various molecules that promote healing.\n                    - **Amyloid Beta**: A peptide that is produced in the brain through the breakdown of a larger protein called amyloid precursor protein (APP). Accumulation of amyloid beta in the brain is commonly associated with neurodegenerative diseases, especially Alzheimer's disease.\n                    - **Alzheimer's Disease**: A progressive neurological disorder characterized by cognitive decline, memory loss, and changes in behavior and personality. It is associated with the accumulation of amyloid beta plaques and neurofibrillary tangles in the brain.\n\n                    ### Relationships\n                    - **Inflammation increases Amyloid Beta**: This relationship indicates that the presence of inflammation in the brain can lead to an increase in the production or aggregation of amyloid beta peptides. This connection suggests that inflammatory processes might play a crucial role in the pathogenesis of Alzheimer's disease by enhancing the toxic effects of amyloid beta.\n                    ###\n                    - **Amyloid Beta accumulates in Alzheimer's Disease**: This relationship illustrates how the accumulation of amyloid beta is a defining feature of Alzheimer's disease. In patients with Alzheimer�s, there is a significant build-up of amyloid beta plaques in the brain, which is believed to contribute to the neurodegenerative processes observed in the disorder, such as synaptic dysfunction and neuronal death.\n                    }\nInicjalizacja ContextBuilderAgent...\nSLSAgent skonfigurowany do użycia PubMed API: True\nAgenci podrzędni (Web, SLS) zainicjalizowani.\n\n=== ContextBuilderAgent: Rozpoczynanie budowania kontekstu ===\nContextBuilderAgent: Pominięto WebSearchAgent (web_results ustawione na pustą listę).\n\n--- Uruchamianie SLSAgent ---\nGenerowanie zapytań dla SLS używając LLM...\nWygenerowane zapytania naukowe przez LLM: ['(\"Inflammation\" [MeSH Terms] OR \"Neuroinflammation\" [MeSH Terms]) AND (\"Amyloid beta-Peptides\" [MeSH Terms] OR \"Amyloid beta-protein precursor\" [MeSH Terms]) AND (\"Alzheimer Disease\" [MeSH Terms])', '(\"Amyloid beta-Peptides\" [MeSH Terms] AND \"Alzheimer Disease\" [MeSH Terms]) AND (\"Inflammation\" [MeSH Terms] OR \"Microglia\" [MeSH Terms] OR \"Astrocytes\" [MeSH Terms]) AND (\"Neurodegeneration\" [MeSH Terms])']\nSLSAgent: Wyszukiwanie w PubMed dla zapytania: '(\"Inflammation\" [MeSH Terms] OR \"Neuroinflammation\" [MeSH Terms]) AND (\"Amyloid beta-Peptides\" [MeSH Terms] OR \"Amyloid beta-protein precursor\" [MeSH Terms]) AND (\"Alzheimer Disease\" [MeSH Terms])'\nWysy³anie zapytania do PubMed (Entrez): (\"Inflammation\" [MeSH Terms] OR \"Neuroinflammation\" [MeSH Terms]) AND (\"Amyloid beta-Peptides\" [MeSH Terms] OR \"Amyloid beta-protein precursor\" [MeSH Terms]) AND (\"Alzheimer Disease\" [MeSH Terms])\nPubMed (Entrez): Znaleziono 5 ID artyku³ów.\nPubMed (Entrez): Pomyœlnie przetworzono 5 artyku³ów.\nDodano 5 artykułów z PubMed dla zapytania '(\"Inflammation\" [MeSH Terms] OR \"Neuroinflammation\" [MeSH Terms]) AND (\"Amyloid beta-Peptides\" [MeSH Terms] OR \"Amyloid beta-protein precursor\" [MeSH Terms]) AND (\"Alzheimer Disease\" [MeSH Terms])'.\nSLSAgent: Wyszukiwanie w PubMed dla zapytania: '(\"Amyloid beta-Peptides\" [MeSH Terms] AND \"Alzheimer Disease\" [MeSH Terms]) AND (\"Inflammation\" [MeSH Terms] OR \"Microglia\" [MeSH Terms] OR \"Astrocytes\" [MeSH Terms]) AND (\"Neurodegeneration\" [MeSH Terms])'\nWysy³anie zapytania do PubMed (Entrez): (\"Amyloid beta-Peptides\" [MeSH Terms] AND \"Alzheimer Disease\" [MeSH Terms]) AND (\"Inflammation\" [MeSH Terms] OR \"Microglia\" [MeSH Terms] OR \"Astrocytes\" [MeSH Terms]) AND (\"Neurodegeneration\" [MeSH Terms])\nPubMed (Entrez): Nie znaleziono artyku³ów.\nDodano 0 artykułów z PubMed dla zapytania '(\"Amyloid beta-Peptides\" [MeSH Terms] AND \"Alzheimer Disease\" [MeSH Terms]) AND (\"Inflammation\" [MeSH Terms] OR \"Microglia\" [MeSH Terms] OR \"Astrocytes\" [MeSH Terms]) AND (\"Neurodegeneration\" [MeSH Terms])'.\nPubMed nie znalazł wyników dla '(\"Amyloid beta-Peptides\" [MeSH Terms] AND \"Alzheimer Disease\" [MeSH Terms]) AND (\"Inflammation\" [MeSH Terms] OR \"Microglia\" [MeSH Terms] OR \"Astrocytes\" [MeSH Terms]) AND (\"Neurodegeneration\" [MeSH Terms])'. Próba za pomocą Firecrawl.\nSLSAgent zakończył pracę, zwracając łącznie 5 potencjalnych artykułów.\nContextBuilderAgent: Otrzymano 5 wyników z SLSAgent.\nContextBuilderAgent: Generowanie zagregowanego kontekstu...\n\n--- Prompt dla generowania zagregowanego kontekstu ---\n\n        Zadanie: Jesteś specjalistą analizującym dane biomedyczne. Na podstawie poniższego **Tekstu Wejściowego** oraz dodatkowych informacji znalezionych w **Internecie** i **Literaturze Naukowej**, wygeneruj szczegółowe, ustrukturyzowane i obiektywne podsumowanie kontekstu w języku angielskim. Zachowaj neutralny ton. Odpowiedź powinna ściśle trzymać się dostarczonych informacji i mieć następującą strukturę:\n\n        **1. Analizowany Temat:**\n        (Krótkie, 1-2 zdaniowe podsumowanie głównych pojęć i relacji opisanych w Tekście Wejściowym.)\n\n        **2. Kluczowe Definicje:**\n        (Wylistuj definicje pojęć zawarte w Tekście Wejściowym. Jeśli definicji brakuje w tekście wejściowym, ale została znaleziona w źródłach, możesz ją dodać z adnotacją źródła, np. [wg. PubMed: ID].)\n        - Pojęcie 1: Definicja...\n        - Pojęcie 2: Definicja...\n        - ...\n\n        **3. Główne Powiązania/Mechanizmy Opisane w Tekście Wejściowym:**\n        (Wylistuj i opisz zwięźle relacje wspomniane w Tekście Wejściowym. Możesz wzbogacić opis o 1-2 zdania kontekstu z znalezionych źródeł, jeśli bezpośrednio dotyczą tej relacji.)\n        - Relacja 1 (np. Zapalenie zwiększa Amyloid Beta): Opis relacji... (ewentualne wzbogacenie ze źródeł)\n        - Relacja 2 (np. Amyloid Beta akumuluje się w Ch. Alzheimera): Opis...\n        - ...\n\n        **4. Kontekst z Literatury Naukowej:**\n        (Syntetyczny przegląd najważniejszych informacji i wniosków z abstraktów naukowych. Pogrupuj informacje tematycznie, jeśli to możliwe. Unikaj bezpośredniego cytowania poszczególnych artykułów, skup się na zagregowanej wiedzy.)\n\n        **5. Kontekst z Ogólnego Internetu:**\n        (Podsumowanie relevantnych informacji z tytułów/snippetów znalezionych w Internecie. Wskaż główne punkty lub perspektywy. Pomiń tę sekcję, jeśli brak wyników lub informacji.)\n\n        **6. Synteza i Wnioski Ogólne:**\n        (Krótkie, 2-4 zdaniowe podsumowanie łączące wszystkie zebrane informacje. Jakie są główne wnioski płynące z analizy tekstu wejściowego w świetle znalezionego kontekstu?)\n\n        --- Dane Wejściowe dla LLM ---\n\n        **Tekst Wejściowy do Analizy:**\n        ```\n        # Ontologist Analysis\n                    {\n                    ### Definitions:\n                    - **Inflammation**: A biological response of body tissues to harmful stimuli, such as pathogens, damaged cells, or irritants. It involves immune cell activation, blood vessel dilation, and the release of various molecules that promote healing.\n                    - **Amyloid Beta**: A peptide that is produced in the brain through the breakdown of a larger protein called amyloid precursor protein (APP). Accumulation of amyloid beta in the brain is commonly associated with neurodegenerative diseases, especially Alzheimer's disease.\n                    - **Alzheimer's Disease**: A progressive neurological disorder characterized by cognitive decline, memory loss, and changes in behavior and personality. It is associated with the accumulation of amyloid beta plaques and neurofibrillary tangles in the brain.\n\n                    ### Relationships\n                    - **Inflammation increases Amyloid Beta**: This relationship indicates that the presence of inflammation in the brain can lead to an increase in the production or aggregation of amyloid beta peptides. This connection suggests that inflammatory processes might play a crucial role in the pathogenesis of Alzheimer's disease by enhancing the toxic effects of amyloid beta.\n                    ###\n                    - **Amyloid Beta accumulates in Alzheimer's Disease**: This relationship illustrates how the accumulation of amyloid beta is a defining feature of Alzheimer's disease. In patients with Alzheimer�s, there is a significant build-up of amyloid beta plaques in the brain, which is believed to contribute to the neurodegenerative processes observed in the disorder, such as synaptic dysfunction and neuronal death.\n                    }\n        ```\n\n        **Informacje z Internetu (Tytuły/Fragmenty):**\n        ```\n        Brak wyników z wyszukiwania w internecie (funkcjonalność wyłączona lub brak wyników).\n        ```\n\n        **Informacje z Literatury Naukowej (Tytuły/Abstrakty):**\n        ```\n        - Associations among Angiotensin-Converting Enzyme, Neuroinflammation, and Cerebrospinal Fluid Biomarkers of Alzheimer's Disease in Non-Dementia Adults. (PMID: 40186068, URL: https://pubmed.ncbi.nlm.nih.gov/40186068/):\nRecent studies have identified the angiotensin-converting enzyme (ACE) gene as a potential candidate influencing Alzheimer's disease (AD) risk. It is crucial to investigate the impact of ACE on AD pathology and its underlying mechanisms. A total of 450 non-demented participants from the Alzheimer's disease Neuroimaging Initiative (ADNI) with data on cerebrospinal fluid (CSF) ACE, AD core biomarkers and inflammation-related biomarkers were included. Multiple linear regression was used to assess the associations among CSF ACE, AD core biomarkers and inflammation-related biomarkers. And we used the mediation models to investigate the potential mechanisms through which ACE influenced AD pathology. The results of multiple linear regression were shown that CSF ACE was significantly correlated with CSF Aβ<sub>42</sub>, P-tau, T-tau (all P < 0.001), and inflammation-related biomarkers (soluble triggering receptor expressed on myeloid cells 2 [sTREM2], progranulin [PGRN], glial fibrillary acidic protein [GFAP], transforming growth factor [TGF]-β1, TGF-β2, TGF-β3, tumor necrosis factor [TNF]-R1, TNF-R2, TNF-α, interleukin [IL]-21, IL-6, IL-7, IL-9, IL-10, IL-12p40, vascular cell adhesion molecule-1 [VCAM-1], and intercellular adhesion molecule-1 [ICAM-1]) (all P < 0.05). In addition, the mediation analysis results showed that the association of CSF ACE and inflammation-related biomarkers (sTREM2, PGRN, TGF-β1, TGF-β2, TNFR1, IL-6, IL-7, IL-9, and VCAM-1) mediated the correlation of CSF Aβ<sub>42</sub> with P-tau. Our findings show that CSF ACE and neuroinflammation are correlated and that their correlation mediates the link between Aβ pathology and P-tau. This suggests ACE may play a significant role in the progression from Aβ pathology to tau pathology.\n\n- Microglial TLR4-Lyn kinase is a critical regulator of neuroinflammation, Aβ phagocytosis, neuronal damage, and cell survival in Alzheimer's disease. (PMID: 40175501, URL: https://pubmed.ncbi.nlm.nih.gov/40175501/):\nDisease-Associated Microglia (DAM) are a focus in Alzheimer's disease (AD) research due to their central involvement in the response to amyloid-beta plaques. Microglial Toll-like receptor 4 (TLR4) is instrumental in the binding of fibrillary amyloid proteins, while Lyn kinase (Lyn) is a member of the Src family of non-receptor tyrosine kinases involved in immune signaling. Lyn is a novel, non-canonical, intracellular adaptor with diverse roles in cell-specific signaling which directly binds to TLR4 to modify its function. Lyn can be activated in response to TLR4 stimulation, leading to phosphorylation of various substrates and modulation of inflammatory and phagocytosis signaling pathways. Here, we investigated the TLR4-Lyn interaction in neuroinflammation using WT, 5XFAD, and 5XFAD x Lyn<sup>-/-</sup> mouse models by western blotting (WB), co-immunoprecipitation (co-IP), immunohistochemistry (IHC) and flow cytometric (FC) analysis. A spatial transcriptomic analysis of microglia in WT, 5XFAD, and 5XFAD x Lyn<sup>-/-</sup> mice revealed essential genes involved in neuroinflammation, Aβ phagocytosis, and neuronal damage. Finally, we explored the effects of a synthetic, TLR4-Lyn modulator protein (TLIM) through an in vitro AD model using primary murine microglia. Our WB, co-IP, IHC, and FC data show an increased, novel, direct protein-protein interaction between TLR4 and Lyn kinase in the brains of 5XFAD mice compared to WT. Furthermore, in the absence of Lyn (5XFAD x Lyn<sup>-/-</sup> mice); increased expression of protective Syk kinase was observed, enhanced microglial Aβ phagocytosis, increased astrocyte activity, decreased neuronal dystrophy, and a further increase in the cell survival signaling and protective DAM population was noted. The DAM population in 5XFAD mice which produce more inflammatory cytokines and phagocytose more Aβ were observed to express greater levels of TLR4 and Lyn. Pathway analysis comparison between WT, 5XFAD, and 5XFAD x Lyn<sup>-/-</sup> mice supported these findings via our microglial spatial transcriptomic analysis. Finally, we created an in vitro co-culture system with primary murine microglial and primary murine hippocampal cells exposed to Aβ as a model of AD. When these co-cultures were treated with our TLR4-Lyn Interaction Modulators (TLIMs), an increase in Aβ phagocytosis and a decrease in neuronal dystrophy was seen. Lyn kinase has a central role in modulating TLR4-induced inflammation and Syk-induced protection in a 5XFAD mouse model. Our TLIMs ameliorate AD sequalae in an in vitro model of AD and could be a promising therapeutic strategy to treat AD.\n\n- IL-17 A Exacerbated Neuroinflammatory and Neurodegenerative Biomarkers in Intranasal Amyloid-Beta Model of Alzheimer's Disease. (PMID: 40163129, URL: https://pubmed.ncbi.nlm.nih.gov/40163129/):\nProinflammatory cytokines, especially interleukin-17 A (IL-17 A) have been found to be significantly associated with AD patients. IL-17 A amplifies neuroinflammation during AD pathology. This study highlighted the ability of IL-17 A to exacerbate amyloid-beta-induced pathology in animals. The AD pathology was induced with repeated intranasal administration of Aβ along with recombinant mouse IL-17 A (rmIL-17) at 1, 2 and 4 µg/kg for seven alternate days. Although, the combination of rmIL-17 and Aβ did not have severe effects on memory of the animals, but it drastically increased the IL-17 A mediated signaling, level of proinflammatory cytokines, oxidative stress and reduced antioxidants in the hippocampus and cortex regions of the animal brains. Interestingly, combining rmIL-17 with Aβ also triggered the expression of AD structural markers like pTau, amyloid-beta and BACE1 in the brain regions. Furthermore, rmIL-17 with Aβ exposure stimulated astrocytes and microglia leading to activation of proinflammatory signaling in the brain of the animals. These results showed the propensity of IL-17 A to promote severity of AD pathology and suggest IL-17 A as potent therapeutic target to control AD progression.\n\n- A Soluble Epoxide Hydrolase Inhibitor Improves Cerebrovascular Dysfunction, Neuroinflammation, Amyloid Burden, and Cognitive Impairments in the hAPP/PS1 TgF344-AD Rat Model of Alzheimer's Disease. (PMID: 40141075, URL: https://pubmed.ncbi.nlm.nih.gov/40141075/):\nAlzheimer's disease (AD) is an increasing global healthcare crisis with few effective treatments. The accumulation of amyloid plaques and hyper-phosphorylated tau are thought to underlie the pathogenesis of AD. However, current studies have recognized a prominent role of cerebrovascular dysfunction in AD. We recently reported that SNPs in soluble epoxide hydrolase (sEH) are linked to AD in human genetic studies and that long-term administration of an sEH inhibitor attenuated cerebral vascular and cognitive dysfunction in a rat model of AD. However, the mechanisms linking changes in cerebral vascular function and neuroprotective actions of sEH inhibitors in AD remain to be determined. This study investigated the effects of administration of an sEH inhibitor, 1-(1-Propanoylpiperidin-4-yl)-3-[4-(trifluoromethoxy)phenyl]urea (TPPU), on neurovascular coupling, blood-brain barrier (BBB) function, neuroinflammation, and cognitive dysfunction in an hAPP/PS1 TgF344-AD rat model of AD. We observed predominant β-amyloid accumulation in the brains of 9-10-month-old AD rats and that TPPU treatment for three months reduced amyloid burden. The functional hyperemic response to whisker stimulation was attenuated in AD rats, and TPPU normalized the response. The sEH inhibitor, TPPU, mitigated capillary rarefaction, BBB leakage, and activation of astrocytes and microglia in AD rats. TPPU increased the expression of pre- and post-synaptic proteins and reduced loss of hippocampal neurons and cognitive impairments in the AD rats, which was confirmed in a transcriptome and GO analysis. These results suggest that sEH inhibitors could be a novel therapeutic strategy for AD.\n\n- The role of n-3-derived specialised pro-resolving mediators (SPMs) in microglial mitochondrial respiration and inflammation resolution in Alzheimer's disease. (PMID: 40114266, URL: https://pubmed.ncbi.nlm.nih.gov/40114266/):\nAlzheimer's disease (AD) is the most common form of dementia globally and is characterised by reduced mitochondrial respiration and cortical deposition of amyloid-β plaques and neurofibrillary tangles comprised of hyper-phosphorylated tau. Despite its characterisation more than 110 years ago, the mechanisms by which AD develops are still unclear. Dysregulation of microglial phagocytosis of amyloid-β may play a key role. Microglia are the major innate immune cell of the central nervous system and are critical responders to pro-inflammatory states. Typically, microglia react with a short-lived inflammatory response. However, a dysregulation in the resolution of this microglial response results in the chronic release of inflammatory mediators. This prolongs the state of neuroinflammation, likely contributing to the pathogenesis of AD. In addition, the microglial specialised pro-resolving mediator (SPM) contribution to phagocytosis of amyloid-β is dysregulated in AD. SPMs are derivatives of dietary n-3 polyunsaturated fatty acids (PUFAs) and potentially represent a strategic target for protection against AD progression. However, there is little understanding of how mitochondrial respiration in microglia may be sustained long term by n-3-derived SPMs, and how this affects their clearance of amyloid-β. Here, we re-evaluate the current literature on SPMs in AD and propose that SPMs may improve phagocytosis of amyloid-β by microglia as a result of sustained mitochondrial respiration and allowing a pro-resolution response.\n        ```\n\n        --- Koniec Danych Wejściowych ---\n\n        Wygenerowane Podsumowanie Kontekstu (w formacie opisanym powyżej):\n        \n--- Koniec Promptu dla generowania zagregowanego kontekstu ---\n\nContextBuilderAgent: Pomyślnie wygenerowano zagregowany kontekst.\n\n=== ContextBuilderAgent: Zakończono budowanie kontekstu (z agregacją) ===\n\n==================================================\n Finalny Kontekst Zosta� Zbudowany\n==================================================\nLiczba wynik�w z wyszukiwania web: 0\nLiczba wynik�w z literatury naukowej: 5\n\nPe�ny kontekst zapisano do pliku: final_context.json\n","output_type":"stream"}],"execution_count":76}]}